{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "from ddpg import *\n",
    "from utils import *\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Concepts\n",
    "\n",
    "* DDPG is adapted specifically for continuous action spaces\n",
    "* Learns an approximator for $Q$ as well as $a^*(s) = \\arg \\max_a Q(s, a)$\n",
    "* Because actions are continuous, we assume that we can differentiate $Q$ w.r.t. $a$.\n",
    "* So, we learn approximator $\\mu: \\mathcal{S} \\rightarrow \\mathcal{A}$ such that $\\max_a Q(s, a) \\approx Q(s, \\mu(s))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6547],\n",
      "        [-1.0437],\n",
      "        [-1.4348]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Checking implementation of value model\n",
    "\n",
    "val_model = ValueModel(3, 1)\n",
    "\n",
    "states = torch.from_numpy(np.array([\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]], dtype=np.float32))\n",
    "actions = torch.from_numpy(np.array([\n",
    "    [7],\n",
    "    [8],\n",
    "    [9]], dtype=np.float32))\n",
    "\n",
    "print(val_model(states, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2539],\n",
      "        [-0.3184],\n",
      "        [-0.5430]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pol_model = PolicyModel(3, 1)\n",
    "\n",
    "print(pol_model(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(val_model, pol_model, val_opt, pol_opt):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eager/miniconda3/envs/ml/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=0, return=-1002.7034, return_avg=-1002.7034\n",
      "ep=1, return=-875.1966, return_avg=-938.9500\n",
      "ep=2, return=-817.7968, return_avg=-898.5656\n",
      "ep=3, return=-1357.7819, return_avg=-1013.3697\n",
      "ep=4, return=-886.2083, return_avg=-987.9374\n",
      "ep=5, return=-1298.9955, return_avg=-1039.7804\n",
      "ep=6, return=-897.8009, return_avg=-1019.4976\n",
      "ep=7, return=-922.8276, return_avg=-1007.4139\n",
      "ep=8, return=-925.9900, return_avg=-998.3668\n",
      "ep=9, return=-1245.8507, return_avg=-1023.1152\n",
      "ep=10, return=-1591.8053, return_avg=-1074.8143\n",
      "ep=11, return=-869.0598, return_avg=-1057.6681\n",
      "ep=12, return=-960.3049, return_avg=-1050.1786\n",
      "ep=13, return=-992.1059, return_avg=-1046.0305\n",
      "ep=14, return=-1074.4443, return_avg=-1047.9248\n",
      "ep=15, return=-1110.5125, return_avg=-1051.8365\n",
      "ep=16, return=-855.2943, return_avg=-1040.2752\n",
      "ep=17, return=-1724.7123, return_avg=-1078.2995\n",
      "ep=18, return=-1003.5343, return_avg=-1074.3645\n",
      "ep=19, return=-1714.6352, return_avg=-1106.3780\n",
      "ep=20, return=-1112.7312, return_avg=-1106.6806\n",
      "ep=21, return=-1205.0666, return_avg=-1111.1526\n",
      "ep=22, return=-1419.7398, return_avg=-1124.5695\n",
      "ep=23, return=-890.4798, return_avg=-1114.8157\n",
      "ep=24, return=-1648.0821, return_avg=-1136.1464\n",
      "Stopping random sampling...\n",
      "ep=25, return=-1044.2654, return_avg=-1132.6125\n",
      "ep=26, return=-942.7515, return_avg=-1125.5806\n",
      "ep=27, return=-1252.3660, return_avg=-1130.1087\n",
      "ep=28, return=-1350.2150, return_avg=-1137.6985\n",
      "ep=29, return=-1203.4414, return_avg=-1139.8900\n",
      "ep=30, return=-1028.2506, return_avg=-1136.2887\n",
      "ep=31, return=-1504.3500, return_avg=-1147.7906\n",
      "ep=32, return=-1211.3035, return_avg=-1149.7152\n",
      "ep=33, return=-974.1922, return_avg=-1144.5528\n",
      "ep=34, return=-781.5935, return_avg=-1134.1825\n",
      "ep=35, return=-797.3011, return_avg=-1124.8247\n",
      "ep=36, return=-266.9717, return_avg=-1101.6395\n",
      "ep=37, return=-1470.0587, return_avg=-1111.3347\n",
      "ep=38, return=-8.4093, return_avg=-1083.0546\n",
      "ep=39, return=-146.0546, return_avg=-1059.6296\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "state_dim = 3\n",
    "action_dim = 1\n",
    "\n",
    "update_after = 1000\n",
    "update_steps = 50\n",
    "num_update_iter = 50\n",
    "\n",
    "max_ep_len = 1000\n",
    "start_steps = 5000\n",
    "batch_size = 1024\n",
    "act_noise = 0.1\n",
    "rho = 0.995\n",
    "discount_rate = 0.99\n",
    "replay_buf = ReplayBuffer(100000)\n",
    "notified = False\n",
    "\n",
    "# the models we're optimizing\n",
    "opt_val_model = ValueModel(state_dim, action_dim)\n",
    "opt_pol_model = PolicyModel(state_dim, action_dim)\n",
    "\n",
    "# the target models\n",
    "tgt_val_model = ValueModel(state_dim, action_dim)\n",
    "tgt_pol_model = PolicyModel(state_dim, action_dim)\n",
    "\n",
    "# freeze weights\n",
    "for param in tgt_val_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in tgt_pol_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# define optimizers\n",
    "val_optim = optim.Adam(opt_val_model.parameters(), lr=0.001)\n",
    "pol_optim = optim.Adam(opt_pol_model.parameters(), lr=0.001)\n",
    "total_r_running_avg = RunningAverage(50)\n",
    "\n",
    "step_cnt = 0\n",
    "for ep in range(2000):\n",
    "    \n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    \n",
    "    for it in range(max_ep_len):\n",
    "        step_cnt += 1\n",
    "        \n",
    "        # move one step\n",
    "        env.render()\n",
    "        \n",
    "        if step_cnt > start_steps and not notified:\n",
    "            print(\"Stopping random sampling...\")\n",
    "            notified = True\n",
    "        \n",
    "        if step_cnt < start_steps:\n",
    "            a = env.action_space.sample()\n",
    "        else:\n",
    "            s_tensor = torch.as_tensor([s]).float()\n",
    "            a = opt_pol_model(s_tensor).detach().numpy()[0]\n",
    "            a += 0.1 * np.random.randn(1)\n",
    "            a = np.clip(a, -2.0, 2.0)\n",
    "            \n",
    "        sp, r, d, _ = env.step(a)\n",
    "        \n",
    "        if it == max_ep_len - 1:\n",
    "            d = True\n",
    "        \n",
    "        d = 1.0 if d else 0.0\n",
    "        \n",
    "        total_r += r\n",
    "        replay_buf.add_one((s, a, [r], sp, [d]))\n",
    "        \n",
    "        # update the current state! for the next prediction!\n",
    "        s = sp\n",
    "\n",
    "        # time to update target networks\n",
    "        if step_cnt % update_steps == 0 and step_cnt > update_after:\n",
    "            for update_iter in range(num_update_iter):\n",
    "                sarsd_arr = list(replay_buf.get_arrays(batch_size))\n",
    "                \n",
    "                val_optim.zero_grad()\n",
    "                \n",
    "                bl = bellman_loss(\n",
    "                    sarsd_arr,\n",
    "                    tgt_val_model,\n",
    "                    tgt_pol_model,\n",
    "                    opt_val_model,\n",
    "                    discount_rate\n",
    "                    )\n",
    "#                 print(\"loss:\", bl.detach().numpy())\n",
    "                \n",
    "                bl.backward()\n",
    "                val_optim.step()\n",
    "                \n",
    "                # freeze Q network\n",
    "                for p in opt_val_model.parameters():\n",
    "                    p.requires_grad = False\n",
    "                \n",
    "                pol_optim.zero_grad()\n",
    "                pl = policy_loss(sarsd_arr, opt_val_model, opt_pol_model)\n",
    "                pl.backward()\n",
    "                pol_optim.step()\n",
    "                \n",
    "                for p in opt_val_model.parameters():\n",
    "                    p.requires_grad = True\n",
    "                \n",
    "                # polyak updating of target networks\n",
    "                with torch.no_grad():\n",
    "                    for opt, tgt in zip(opt_val_model.parameters(), tgt_val_model.parameters()):\n",
    "                        tgt.data.mul_(rho)\n",
    "                        tgt.data.add_((1.0 - rho) * opt.data)\n",
    "                        \n",
    "                    for opt, tgt in zip(opt_pol_model.parameters(), tgt_pol_model.parameters()):\n",
    "                        tgt.data.mul_(rho)\n",
    "                        tgt.data.add_((1.0 - rho) * opt.data)\n",
    "                        \n",
    "#                     tgt_val_new_state = OrderedDict()\n",
    "#                     for layer, val in tgt_val_model.state_dict().items():\n",
    "#                         tgt_val_new_state[layer] = (\n",
    "#                             rho * val +\n",
    "#                             (1 - rho) * opt_val_model.state_dict()[layer]\n",
    "#                             )\n",
    "#                     tgt_val_model.load_state_dict(tgt_val_new_state, strict=False)\n",
    "                    \n",
    "#                     tgt_pol_new_state = OrderedDict()\n",
    "#                     for layer, val in tgt_pol_model.state_dict().items():\n",
    "#                         tgt_pol_new_state[layer] = (\n",
    "#                             rho * val +\n",
    "#                             (1 - rho) * opt_pol_model.state_dict()[layer]\n",
    "#                             )\n",
    "#                     tgt_pol_model.load_state_dict(tgt_pol_new_state, strict=False)\n",
    "                        \n",
    "        if d:\n",
    "            break\n",
    "    \n",
    "    total_r_running_avg.update(total_r)\n",
    "    print(f\"ep={ep}, return={total_r:0.4f}, return_avg={total_r_running_avg.avg():0.4f}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(opt_val_model.state_dict(), \"./value_model.pth\")\n",
    "#torch.save(opt_pol_model.state_dict(), \"./policy_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "s_t = test_env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    test_env.render()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        a = opt_pol_model(torch.as_tensor([s_t]).float()).detach().numpy()[0]\n",
    "        a = np.clip(a, -2.0, 2.0)\n",
    "    \n",
    "    sp_t, r_t, d_t, _ = test_env.step(a)\n",
    "    \n",
    "    s_t = sp_t\n",
    "    \n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
