{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "\n",
    "from ddpg import *\n",
    "from utils import *\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Concepts\n",
    "\n",
    "* DDPG is adapted specifically for continuous action spaces\n",
    "* Learns an approximator for $Q$ as well as $a^*(s) = \\arg \\max_a Q(s, a)$\n",
    "* Because actions are continuous, we assume that we can differentiate $Q$ w.r.t. $a$.\n",
    "* So, we learn approximator $\\mu: \\mathcal{S} \\rightarrow \\mathcal{A}$ such that $\\max_a Q(s, a) \\approx Q(s, \\mu(s))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3019],\n",
      "        [0.1899],\n",
      "        [0.0780]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Checking implementation of value model\n",
    "\n",
    "val_model = ValueModel(3, 1)\n",
    "\n",
    "states = torch.from_numpy(np.array([\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]], dtype=np.float32))\n",
    "actions = torch.from_numpy(np.array([\n",
    "    [7],\n",
    "    [8],\n",
    "    [9]], dtype=np.float32))\n",
    "\n",
    "print(val_model(states, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0208],\n",
      "        [0.1753],\n",
      "        [0.3622]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "pol_model = PolicyModel(3, 1)\n",
    "\n",
    "print(pol_model(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(val_model, pol_model, val_opt, pol_opt):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eager/miniconda3/envs/ml/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=0, return=-1326.1620, return_avg=-1326.1620\n",
      "ep=1, return=-966.2772, return_avg=-1146.2196\n",
      "ep=2, return=-1656.6655, return_avg=-1316.3682\n",
      "ep=3, return=-886.0697, return_avg=-1208.7936\n",
      "ep=4, return=-979.1414, return_avg=-1162.8632\n",
      "ep=5, return=-983.3390, return_avg=-1132.9425\n",
      "ep=6, return=-1304.5672, return_avg=-1157.4603\n",
      "ep=7, return=-1280.0197, return_avg=-1172.7802\n",
      "ep=8, return=-940.7424, return_avg=-1146.9982\n",
      "ep=9, return=-1678.6912, return_avg=-1200.1675\n",
      "ep=10, return=-1735.3694, return_avg=-1248.8222\n",
      "ep=11, return=-827.6727, return_avg=-1213.7264\n",
      "ep=12, return=-964.5256, return_avg=-1194.5571\n",
      "ep=13, return=-1326.8698, return_avg=-1204.0081\n",
      "ep=14, return=-1057.9766, return_avg=-1194.2726\n",
      "ep=15, return=-1749.2966, return_avg=-1228.9616\n",
      "ep=16, return=-1191.5632, return_avg=-1226.7617\n",
      "ep=17, return=-1182.8795, return_avg=-1224.3238\n",
      "ep=18, return=-1385.1956, return_avg=-1232.7908\n",
      "ep=19, return=-1560.3901, return_avg=-1249.1707\n",
      "ep=20, return=-1523.6518, return_avg=-1262.2412\n",
      "ep=21, return=-1724.1986, return_avg=-1283.2393\n",
      "ep=22, return=-875.0080, return_avg=-1265.4901\n",
      "ep=23, return=-1511.0701, return_avg=-1275.7226\n",
      "ep=24, return=-1754.3441, return_avg=-1294.8675\n",
      "ep=25, return=-1302.7326, return_avg=-1295.1700\n",
      "ep=26, return=-1257.5459, return_avg=-1293.7765\n",
      "ep=27, return=-1028.1080, return_avg=-1284.2883\n",
      "ep=28, return=-1664.5380, return_avg=-1297.4004\n",
      "ep=29, return=-1774.9714, return_avg=-1313.3194\n",
      "ep=30, return=-1282.7530, return_avg=-1312.3334\n",
      "ep=31, return=-1659.7991, return_avg=-1323.1917\n",
      "ep=32, return=-1090.5984, return_avg=-1316.1434\n",
      "ep=33, return=-627.9398, return_avg=-1295.9022\n",
      "ep=34, return=-1209.1754, return_avg=-1293.4242\n",
      "ep=35, return=-1264.6006, return_avg=-1292.6236\n",
      "ep=36, return=-1042.0101, return_avg=-1285.8503\n",
      "ep=37, return=-1619.4842, return_avg=-1294.6301\n",
      "ep=38, return=-1825.6329, return_avg=-1308.2456\n",
      "ep=39, return=-1274.2192, return_avg=-1307.3949\n",
      "ep=40, return=-1337.9497, return_avg=-1308.1401\n",
      "ep=41, return=-1061.7810, return_avg=-1302.2744\n",
      "ep=42, return=-1167.9598, return_avg=-1299.1508\n",
      "ep=43, return=-1685.3365, return_avg=-1307.9278\n",
      "ep=44, return=-1131.4931, return_avg=-1304.0070\n",
      "ep=45, return=-1148.5701, return_avg=-1300.6280\n",
      "ep=46, return=-1176.5424, return_avg=-1297.9878\n",
      "ep=47, return=-1069.7772, return_avg=-1293.2334\n",
      "ep=48, return=-1233.9178, return_avg=-1292.0229\n",
      "ep=49, return=-1477.3533, return_avg=-1295.7295\n",
      "ep=50, return=-1175.7882, return_avg=-1293.3777\n",
      "ep=51, return=-1684.0064, return_avg=-1300.8898\n",
      "ep=52, return=-1172.7852, return_avg=-1298.4728\n",
      "ep=53, return=-1146.4572, return_avg=-1295.6577\n",
      "ep=54, return=-1363.8566, return_avg=-1296.8976\n",
      "ep=55, return=-1150.3022, return_avg=-1294.2799\n",
      "ep=56, return=-1153.6898, return_avg=-1291.8134\n",
      "ep=57, return=-1698.1274, return_avg=-1298.8188\n",
      "ep=58, return=-1572.0033, return_avg=-1303.4490\n",
      "ep=59, return=-870.7788, return_avg=-1296.2379\n",
      "ep=60, return=-925.5304, return_avg=-1290.1607\n",
      "ep=61, return=-880.4437, return_avg=-1283.5524\n",
      "ep=62, return=-1841.6640, return_avg=-1292.4113\n",
      "ep=63, return=-970.6318, return_avg=-1287.3835\n",
      "ep=64, return=-729.0114, return_avg=-1278.7931\n",
      "ep=65, return=-1415.2684, return_avg=-1280.8609\n",
      "ep=66, return=-1079.5359, return_avg=-1277.8561\n",
      "ep=67, return=-1166.9940, return_avg=-1276.2258\n",
      "ep=68, return=-1044.4680, return_avg=-1272.8669\n",
      "ep=69, return=-970.0380, return_avg=-1268.5408\n",
      "ep=70, return=-1216.0955, return_avg=-1267.8021\n",
      "ep=71, return=-1039.5662, return_avg=-1264.6322\n",
      "ep=72, return=-749.6941, return_avg=-1257.5783\n",
      "ep=73, return=-1061.2491, return_avg=-1254.9252\n",
      "ep=74, return=-1512.0096, return_avg=-1258.3530\n",
      "ep=75, return=-1050.9304, return_avg=-1255.6237\n",
      "ep=76, return=-1535.5777, return_avg=-1259.2595\n",
      "ep=77, return=-1529.7314, return_avg=-1262.7271\n",
      "ep=78, return=-1017.8664, return_avg=-1259.6276\n",
      "ep=79, return=-871.0149, return_avg=-1254.7699\n",
      "ep=80, return=-1814.1781, return_avg=-1261.6762\n",
      "ep=81, return=-1305.7420, return_avg=-1262.2136\n",
      "ep=82, return=-966.3263, return_avg=-1258.6487\n",
      "ep=83, return=-1606.0363, return_avg=-1262.7842\n",
      "ep=84, return=-1368.5871, return_avg=-1264.0290\n",
      "ep=85, return=-1409.3638, return_avg=-1265.7189\n",
      "ep=86, return=-1102.9717, return_avg=-1263.8483\n",
      "ep=87, return=-1206.6296, return_avg=-1263.1980\n",
      "ep=88, return=-1154.3708, return_avg=-1261.9753\n",
      "ep=89, return=-898.1701, return_avg=-1257.9330\n",
      "ep=90, return=-1181.9237, return_avg=-1257.0977\n",
      "ep=91, return=-1488.2126, return_avg=-1259.6098\n",
      "ep=92, return=-1325.7006, return_avg=-1260.3205\n",
      "ep=93, return=-1208.7410, return_avg=-1259.7718\n",
      "ep=94, return=-847.1256, return_avg=-1255.4281\n",
      "ep=95, return=-1060.8818, return_avg=-1253.4016\n",
      "ep=96, return=-1251.2146, return_avg=-1253.3791\n",
      "ep=97, return=-970.7015, return_avg=-1250.4946\n",
      "ep=98, return=-1057.8062, return_avg=-1248.5482\n",
      "ep=99, return=-867.4871, return_avg=-1244.7376\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-62166414975e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# move one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep_cnt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstart_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/gym/envs/classic_control/pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# TODO canvas.flip?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sync_resize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.7/site-packages/pyglet/gl/xlib.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vsync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_vsync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mglx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglXSwapBuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_display\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglx_window\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "state_dim = 3\n",
    "action_dim = 1\n",
    "update_after = 1000\n",
    "update_steps = 500\n",
    "max_ep_len = 1000\n",
    "start_steps = 5000\n",
    "num_update_iter = 10\n",
    "batch_size = 256\n",
    "act_noise = 0.1\n",
    "rho = 0.995\n",
    "discount_rate = 0.99\n",
    "replay_buf = ReplayBuffer(1000000)\n",
    "\n",
    "# the models we're optimizing\n",
    "opt_val_model = ValueModel(state_dim, action_dim)\n",
    "opt_pol_model = PolicyModel(state_dim, action_dim)\n",
    "\n",
    "# the target models\n",
    "tgt_val_model = ValueModel(state_dim, action_dim)\n",
    "tgt_pol_model = PolicyModel(state_dim, action_dim)\n",
    "\n",
    "# freeze weights\n",
    "for param in tgt_val_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in tgt_pol_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# define optimizers\n",
    "val_optim = optim.Adam(opt_val_model.parameters(), lr=0.001)\n",
    "pol_optim = optim.Adam(opt_pol_model.parameters(), lr=0.0001)\n",
    "total_r_running_avg = RunningAverage(100)\n",
    "\n",
    "step_cnt = 0\n",
    "for ep in range(2000):\n",
    "    \n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    \n",
    "    for _ in range(max_ep_len):\n",
    "        step_cnt += 1\n",
    "        \n",
    "        # move one step\n",
    "        env.render()\n",
    "        \n",
    "        if step_cnt < start_steps:\n",
    "            a = torch.from_numpy(env.action_space.sample().astype(np.float32))\n",
    "        else:\n",
    "            a = opt_pol_model(\n",
    "                torch.from_numpy(np.array([s], dtype=np.float32))\n",
    "                ).detach()\n",
    "            noise = np.random.randn() * 0.1\n",
    "            a += noise\n",
    "            \n",
    "        sp, r, d, _ = env.step(a)\n",
    "        d = 1.0 if d else 0.0\n",
    "        total_r += r.detach().item()\n",
    "        replay_buf.add_one((s, a, r, sp, d))\n",
    "\n",
    "        # time to update target networks\n",
    "        if step_cnt % update_steps == 0 and step_cnt > update_after:\n",
    "            for update_iter in range(num_update_iter):\n",
    "                sarsd_arr = list(replay_buf.get_arrays(batch_size))\n",
    "                \n",
    "                val_optim.zero_grad()\n",
    "                pol_optim.zero_grad()\n",
    "                \n",
    "                bl = bellman_loss(\n",
    "                    sarsd_arr,\n",
    "                    tgt_val_model,\n",
    "                    tgt_pol_model,\n",
    "                    opt_val_model,\n",
    "                    discount_rate\n",
    "                    )\n",
    "                \n",
    "                bl.backward()\n",
    "                val_optim.step()\n",
    "                \n",
    "                pl = policy_loss(sarsd_arr, opt_val_model, opt_pol_model)\n",
    "                \n",
    "                pl.backward()\n",
    "                pol_optim.step()\n",
    "                \n",
    "                # polyak updating of target networks\n",
    "                with torch.no_grad():\n",
    "                    tgt_val_new_state = OrderedDict()\n",
    "                    for layer, val in tgt_val_model.state_dict().items():\n",
    "                        tgt_val_new_state[layer] = (\n",
    "                            rho * val +\n",
    "                            (1 - rho) * opt_val_model.state_dict()[layer]\n",
    "                            )\n",
    "                    tgt_val_model.load_state_dict(tgt_val_new_state, strict=False)\n",
    "                    \n",
    "                    tgt_pol_new_state = OrderedDict()\n",
    "                    for layer, val in tgt_pol_model.state_dict().items():\n",
    "                        tgt_pol_new_state[layer] = (\n",
    "                            rho * val +\n",
    "                            (1 - rho) * opt_pol_model.state_dict()[layer]\n",
    "                            )\n",
    "                    tgt_pol_model.load_state_dict(tgt_pol_new_state, strict=False)\n",
    "                        \n",
    "        if d:\n",
    "            break\n",
    "    \n",
    "    total_r_running_avg.update(total_r)\n",
    "    print(f\"ep={ep}, return={total_r:0.4f}, return_avg={total_r_running_avg.avg():0.4f}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['f1.weight', 'f1.bias', 'f2.weight', 'f2.bias'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_val_model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
